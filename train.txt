WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 2): env://
| distributed init (rank 1): env://
| distributed init (rank 0): env://
| distributed init (rank 3): env://
Namespace(aa='rand-m9-mstd0.5-inc1', batch_size=128, clip_grad=0.01, clip_mode='agc', color_jitter=0.4, cooldown_epochs=10, cutmix=1.0, cutmix_minmax=None, data_path='/data1/datasets/imagenet', data_set='IMNET', decay_epochs=30, decay_rate=0.1, device='cuda', dist_backend='nccl', dist_eval=True, dist_url='env://', distillation_alpha=0.5, distillation_tau=1.0, distillation_type='hard', distributed=True, epochs=2, eval=False, finetune='', gpu=0, inat_category='name', input_size=224, lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, min_lr=1e-05, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='efficientformer_l1', model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, momentum=0.9, num_workers=8, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='efficientformer_l1_2d', patience_epochs=10, pin_mem=True, rank=0, recount=1, remode='pixel', repeated_aug=True, reprob=0.25, resplit=False, resume='', sched='cosine', seed=0, smoothing=0.1, start_epoch=0, sync_bn=True, teacher_model='regnety_160', teacher_path='https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth', train_interpolation='bicubic', warmup_epochs=5, warmup_lr=1e-05, weight_decay=0.025, world_size=4)
/data1/dxw/conda_environment/efficientformer/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/data1/dxw/conda_environment/efficientformer/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/data1/dxw/conda_environment/efficientformer/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/data1/dxw/conda_environment/efficientformer/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
Creating model: efficientformer_l1
number of params: 12289928
Creating teacher model: regnety_160
Start training for 2 epochs
Epoch: [0]  [   0/2502]  eta: 11:48:06  lr: 0.000010  loss: 6.9365 (6.9365)  time: 16.9809  data: 3.0549  max mem: 18045
Epoch: [0]  [ 100/2502]  eta: 0:44:22  lr: 0.000010  loss: 6.9136 (6.9163)  time: 0.9401  data: 0.0002  max mem: 18045
Epoch: [0]  [ 200/2502]  eta: 0:39:36  lr: 0.000010  loss: 6.9030 (6.9114)  time: 0.9355  data: 0.0002  max mem: 18045
Epoch: [0]  [ 300/2502]  eta: 0:36:52  lr: 0.000010  loss: 6.8926 (6.9072)  time: 0.9460  data: 0.0002  max mem: 18045
Epoch: [0]  [ 400/2502]  eta: 0:34:42  lr: 0.000010  loss: 6.8913 (6.9044)  time: 0.9520  data: 0.0002  max mem: 18045
Epoch: [0]  [ 500/2502]  eta: 0:32:44  lr: 0.000010  loss: 6.8885 (6.9015)  time: 0.9307  data: 0.0002  max mem: 18045
Epoch: [0]  [ 600/2502]  eta: 0:30:55  lr: 0.000010  loss: 6.8830 (6.8989)  time: 0.9388  data: 0.0002  max mem: 18045
Epoch: [0]  [ 700/2502]  eta: 0:29:08  lr: 0.000010  loss: 6.8819 (6.8960)  time: 0.9537  data: 0.0002  max mem: 18045
Epoch: [0]  [ 800/2502]  eta: 0:27:25  lr: 0.000010  loss: 6.8722 (6.8936)  time: 0.9583  data: 0.0002  max mem: 18045
Epoch: [0]  [ 900/2502]  eta: 0:25:44  lr: 0.000010  loss: 6.8614 (6.8906)  time: 0.9593  data: 0.0002  max mem: 18045
Epoch: [0]  [1000/2502]  eta: 0:24:03  lr: 0.000010  loss: 6.8711 (6.8884)  time: 0.9440  data: 0.0002  max mem: 18045
Epoch: [0]  [1100/2502]  eta: 0:22:24  lr: 0.000010  loss: 6.8599 (6.8861)  time: 0.9703  data: 0.0002  max mem: 18045
Epoch: [0]  [1200/2502]  eta: 0:20:46  lr: 0.000010  loss: 6.8576 (6.8840)  time: 0.9592  data: 0.0002  max mem: 18045
Epoch: [0]  [1300/2502]  eta: 0:19:08  lr: 0.000010  loss: 6.8514 (6.8819)  time: 0.9376  data: 0.0002  max mem: 18045
Epoch: [0]  [1400/2502]  eta: 0:17:31  lr: 0.000010  loss: 6.8517 (6.8798)  time: 0.9362  data: 0.0002  max mem: 18045
Epoch: [0]  [1500/2502]  eta: 0:15:56  lr: 0.000010  loss: 6.8415 (6.8776)  time: 0.9597  data: 0.0186  max mem: 18045
Epoch: [0]  [1600/2502]  eta: 0:14:20  lr: 0.000010  loss: 6.8470 (6.8758)  time: 0.9449  data: 0.0002  max mem: 18045
Epoch: [0]  [1700/2502]  eta: 0:12:46  lr: 0.000010  loss: 6.8420 (6.8737)  time: 0.9602  data: 0.0090  max mem: 18045
Epoch: [0]  [1800/2502]  eta: 0:11:11  lr: 0.000010  loss: 6.8431 (6.8718)  time: 0.9660  data: 0.0002  max mem: 18045
Epoch: [0]  [1900/2502]  eta: 0:09:37  lr: 0.000010  loss: 6.8285 (6.8701)  time: 1.0599  data: 0.1175  max mem: 18045
Epoch: [0]  [2000/2502]  eta: 0:08:02  lr: 0.000010  loss: 6.8334 (6.8681)  time: 1.0158  data: 0.0429  max mem: 18045
Epoch: [0]  [2100/2502]  eta: 0:06:27  lr: 0.000010  loss: 6.8381 (6.8667)  time: 0.9317  data: 0.0002  max mem: 18045
Epoch: [0]  [2200/2502]  eta: 0:04:51  lr: 0.000010  loss: 6.8342 (6.8654)  time: 0.9822  data: 0.0655  max mem: 18045
Epoch: [0]  [2300/2502]  eta: 0:03:15  lr: 0.000010  loss: 6.8314 (6.8640)  time: 1.0334  data: 0.1207  max mem: 18045
Epoch: [0]  [2400/2502]  eta: 0:01:39  lr: 0.000010  loss: 6.8309 (6.8627)  time: 1.0904  data: 0.1035  max mem: 18045
Epoch: [0]  [2500/2502]  eta: 0:00:01  lr: 0.000010  loss: 6.8311 (6.8614)  time: 0.9436  data: 0.0005  max mem: 18045
Epoch: [0]  [2501/2502]  eta: 0:00:00  lr: 0.000010  loss: 6.8311 (6.8614)  time: 0.9429  data: 0.0005  max mem: 18045
Epoch: [0] Total time: 0:40:34 (0.9729 s / it)
Averaged stats: lr: 0.000010  loss: 6.8311 (6.8614)
Epoch: [1]  [   0/2502]  eta: 4:25:25  lr: 0.000010  loss: 6.7803 (6.7803)  time: 6.3651  data: 5.3699  max mem: 18045
Epoch: [1]  [ 100/2502]  eta: 0:40:16  lr: 0.000010  loss: 6.8230 (6.8283)  time: 0.9601  data: 0.0002  max mem: 18045
Epoch: [1]  [ 200/2502]  eta: 0:37:20  lr: 0.000010  loss: 6.8330 (6.8299)  time: 0.9531  data: 0.0002  max mem: 18045
Epoch: [1]  [ 300/2502]  eta: 0:35:26  lr: 0.000010  loss: 6.8263 (6.8295)  time: 0.9651  data: 0.0002  max mem: 18045
Epoch: [1]  [ 400/2502]  eta: 0:33:44  lr: 0.000010  loss: 6.8222 (6.8284)  time: 0.9625  data: 0.0002  max mem: 18045
Epoch: [1]  [ 500/2502]  eta: 0:32:00  lr: 0.000010  loss: 6.8261 (6.8277)  time: 0.9354  data: 0.0003  max mem: 18045
Epoch: [1]  [ 600/2502]  eta: 0:30:19  lr: 0.000010  loss: 6.8325 (6.8275)  time: 0.9550  data: 0.0002  max mem: 18045
Epoch: [1]  [ 700/2502]  eta: 0:28:40  lr: 0.000010  loss: 6.8202 (6.8268)  time: 0.9294  data: 0.0002  max mem: 18045
Epoch: [1]  [ 800/2502]  eta: 0:27:05  lr: 0.000010  loss: 6.8210 (6.8259)  time: 0.9429  data: 0.0003  max mem: 18045
Epoch: [1]  [ 900/2502]  eta: 0:25:28  lr: 0.000010  loss: 6.8231 (6.8255)  time: 0.9318  data: 0.0002  max mem: 18045
Epoch: [1]  [1000/2502]  eta: 0:23:51  lr: 0.000010  loss: 6.8238 (6.8248)  time: 0.9231  data: 0.0002  max mem: 18045
Epoch: [1]  [1100/2502]  eta: 0:22:13  lr: 0.000010  loss: 6.8208 (6.8241)  time: 0.9229  data: 0.0002  max mem: 18045
Epoch: [1]  [1200/2502]  eta: 0:20:36  lr: 0.000010  loss: 6.8224 (6.8241)  time: 0.9540  data: 0.0002  max mem: 18045
Epoch: [1]  [1300/2502]  eta: 0:19:00  lr: 0.000010  loss: 6.8108 (6.8237)  time: 0.9482  data: 0.0002  max mem: 18045
Epoch: [1]  [1400/2502]  eta: 0:17:05  lr: 0.000010  loss: 6.8124 (6.8232)  time: 0.7904  data: 0.3017  max mem: 18045
Epoch: [1]  [1500/2502]  eta: 0:15:18  lr: 0.000010  loss: 6.8077 (6.8225)  time: 0.7119  data: 0.2365  max mem: 18045
Epoch: [1]  [1600/2502]  eta: 0:13:34  lr: 0.000010  loss: 6.8000 (6.8220)  time: 0.7188  data: 0.2531  max mem: 18045
Epoch: [1]  [1700/2502]  eta: 0:11:53  lr: 0.000010  loss: 6.8119 (6.8214)  time: 0.6640  data: 0.1744  max mem: 18045
Epoch: [1]  [1800/2502]  eta: 0:10:17  lr: 0.000010  loss: 6.8080 (6.8210)  time: 0.6806  data: 0.1946  max mem: 18045
Epoch: [1]  [1900/2502]  eta: 0:08:43  lr: 0.000010  loss: 6.8134 (6.8207)  time: 0.7722  data: 0.3022  max mem: 18045
Epoch: [1]  [2000/2502]  eta: 0:07:13  lr: 0.000010  loss: 6.8156 (6.8203)  time: 0.7687  data: 0.3153  max mem: 18045
Epoch: [1]  [2100/2502]  eta: 0:05:43  lr: 0.000010  loss: 6.8122 (6.8199)  time: 0.7486  data: 0.2678  max mem: 18045
Epoch: [1]  [2200/2502]  eta: 0:04:16  lr: 0.000010  loss: 6.8080 (6.8196)  time: 0.7603  data: 0.2154  max mem: 18045
Epoch: [1]  [2300/2502]  eta: 0:02:50  lr: 0.000010  loss: 6.7986 (6.8191)  time: 0.7918  data: 0.2412  max mem: 18045
Epoch: [1]  [2400/2502]  eta: 0:01:25  lr: 0.000010  loss: 6.7992 (6.8186)  time: 0.7152  data: 0.1659  max mem: 18045
Epoch: [1]  [2500/2502]  eta: 0:00:01  lr: 0.000010  loss: 6.7982 (6.8180)  time: 0.6571  data: 0.1039  max mem: 18045
Epoch: [1]  [2501/2502]  eta: 0:00:00  lr: 0.000010  loss: 6.7982 (6.8180)  time: 0.6558  data: 0.1039  max mem: 18045
Epoch: [1] Total time: 0:34:44 (0.8331 s / it)
Averaged stats: lr: 0.000010  loss: 6.7982 (6.8182)
Training time 1:15:19
